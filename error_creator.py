import numpy as np
import re
from datasets import load_dataset
import os
from tqdm import tqdm
import argparse


fixed_random_words='''родрпАро╡ро┐ройрпБроХрпНроХрпЛро░рпН рокро╛ро▓роорпН роЕроорпИрокрпНрокрпЛроорпН -роороХро╛роХро╡ро┐ Welcome to delegates of Bharathi International роирпАро▓ ро╡рогрпНрогродрпНродро┐ро▓рпН роОро┤рпБродрпНродрпБроХрпНроХро│рпН ро╡рпЖро│рпНро│рпИродрпН ...
родрпЖройрпНройро▓рпН роХро╛ро▒рпНро▒рпБ ро╡рпАроЪрпБро╡родрпБ роиро┐ройрпНро▒рпБ роЪрпБрооро╛ро░рпН роТро░рпБ рооро╛родроХро╛ро▓рооро╛ропро┐ро▒рпНро▒рпБ. роорпБройрпНройро╛ро│рпН роЬройро╛родро┐рокродро┐ рооро╣ро┐роирпНрод ро░ро╛роЬрокроХрпНро╖ро╡ро┐ройро╛ро▓рпН роорпБройрпНройрпЖроЯрпБроХрпНроХрокрпНрокроЯрпНроЯ рокрпЗро╛ро░ро╛роЯрпНроЯроорпН роЙроЯрпНрокроЯ ро╡рпЗро▓рпИроиро┐ро▒рпБродрпНрод рокрпЗро╛ро░ро╛роЯрпНроЯроЩрпНроХро│рпБроХрпНроХро╛рой роиро┐родро┐ роЕройрпБроЪро░рогрпИропрпИ роЪрпАройро╛ро╡рпЗ ро╡ро┤роЩрпНроХро┐ роиро╛роЯрпНроЯрпИропрпБроорпН роЕро░роЪро╛роЩрпНроХродрпНродрпИропрпБроорпН роирпЖро░рпБроХрпНроХроЯро┐роХрпНроХрпБро│рпНро│ро╛роХрпНроХ роорпБропро▓рпНроХро┐ро▒родрпБ роОрой роЪроорпВроХ роиро▓ройрпНрокрпБро░ро┐ рокро┐ро░родро┐ роЕроорпИроЪрпНроЪро░рпН ро░роЮрпНроЪройрпН ро░ро╛роороиро╛ропроХрпНроХ родрпЖро░ро┐ро╡ро┐родрпНродро╛ро░рпН.
роирпЗро▒рпНро▒рпБ роорпБройрпНройро╛ро│рпН роЬройро╛родро┐рокродро┐ рооро╣ро┐роирпНрод ро░ро╛роЬрокроХрпНро╖ро╡ро┐ройрпН родро▓рпИроорпИропро┐ро▓рпН рокрпЗро╛ро░ро╛роЯрпНроЯроорпН роорпБройрпНройрпЖроЯрпБроХрпНроХрокрпНрокроЯрпНроЯ рокрпЗро╛ро░ро╛роЯрпНроЯродрпНродро┐ройрпН рокро┐ройрпНройрогро┐ропро┐ро▓рпН роЪрпАройро╛ро╡рпЗ роЙро│рпНро│родрпБ
рокрпЗро╛ро░ро╛роЯрпНроЯроЩрпНроХро│рпН роироЯродрпНродрпБроорпН роЕро│ро╡рпБроХрпНроХрпБ рооро╣ро┐роирпНрод роЕрогро┐ропро┐ройро░рпБроХрпНроХрпБ роОроЩрпНроХро┐ро░рпБроирпНродрпБ рокрогроорпН ро╡ро░рпБроХро┐ройрпНро▒родрпБ
роЖроХро╡рпЗ рооро╣ро┐роирпНрод роЕрогро┐ропро┐ройро░рпН рокрпЗро░рогро┐роХрпНроХрпБроорпН роиро╛роЯрпНроЯро┐ро▓рпН роироЯродрпНродрокрпНрокроЯрпБроорпН ро╡рпЗро▓рпИроиро┐ро▒рпБродрпНрод рокрпЗро╛ро░ро╛роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБроорпН роЪрпАройро╛ро╡рпЗ роиро┐родро┐ ро╡ро┤роЩрпНроХрпБроХро┐ройрпНро▒родрпБ.
роЗро▓роЩрпНроХрпИропро┐ро▓рпН роирпЖро░рпБроХрпНроХроЯро┐ропро╛рой роиро┐ро▓рпИроорпИропрпИ роПро▒рпНрокроЯрпБродрпНродро╡рпЗ роЪрпАройро╛ роЗро╡рпНро╡ро╛ро▒рпБ роЪрпЖропро▒рпНрокроЯрпНроЯрпБ ро╡ро░рпБроХро┐ройрпНро▒родрпБ
роЪрпАройро╛ ро╡ро┐ро╡роХро╛ро░родрпНродро┐ро▓рпН роХро┤рпБродрпНродрпБ роироЪрпБроХрпНроХрокрпНрокроЯрпНроЯ роиро┐ро▓рпИроорпИропро┐ро▓рпЗропрпЗ роиро╛роорпН роЙро│рпНро│рпЗро╛роорпН
роЖроХро╡рпЗ роОроородрпБ роиро╛роЯрпНроЯрпИ роирпЖро░рпБроХрпНроХроЯро┐роХрпНроХрпБ роЙро│рпНро│ро╛роХрпНроХ ро╡рпЗрогрпНроЯро╛роорпН роОрой роЪрпАройро╛ро╡ро┐роЯроорпН роиро╛ройрпН родро▓рпИроХрпБройро┐роирпНродрпБ ро╡рпЗрогрпНроЯрпБроХро┐ройрпНро▒рпЗройрпН.
роЕродрпНродрпБроЯройрпН рокрпЖро╛ройрпНроЪрпЗроХро╛ро╡рпБроХрпНроХрпБ роЕро░рпБроХро┐ро▓рпН роЗро░рпБроирпНрод рокро╛родро╛ро│ роХрпЗро╛ро╖рпНроЯро┐ роТро░рпБро╡ро░рпИ роХрпИродрпБ роЪрпЖропрпНродродрпИ рокрпЗро╛ройрпНро▒рпБ роПройрпИроп роЕроорпИроЪрпНроЪро░рпНроХро│ро┐ройрпН рокродро╛ро│ роХрпЗро╛ро╖рпНроЯро┐ропро┐ройро░рпИ роХрпИродрпБ роЪрпЖропрпНроп ро╡рпЗрогрпНроЯрпБроорпН роОройрпНро▒рпБроорпН роЕро╡ро░рпН роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯро╛ро░рпН.
родро┐ро░рпБроородро┐'''
fixed_random_words=fixed_random_words.split(' ')
 
parser = argparse.ArgumentParser()
 
parser.add_argument("-sd", "--savedir", help = "dataset directory", default='dump/tamil2/')
parser.add_argument("-n", "--ndata", help = "size of dataset", default=10, type=int)
# 0 for tamil 1 for english
parser.add_argument("-lang", "--language", help = "Choose the dataset language", default=0, type=int, choices=[0, 1])
 
args = parser.parse_args()
 

def corrupt_homophones(data):
  output_data=""
  for i in data:
    x=np.random.randint(5, size=1)[0]

    if i=="ро▓":output_data += ["ро│", "ро┤", "ро▓", "ро▓", "ро▓"][x]
      
    elif i=="ро│":output_data += ["ро▓", "ро┤", "ро│", "ро│", "ро│"][x]

    elif i=="ро┤":output_data += ["ро▓", "ро│", "ро┤", "ро┤", "ро┤"][x]

    elif i=="рои":output_data += ["рог", "рой", "рои", "рои", "рои"][x]

    elif i=="рой":output_data += ["рог", "рои", "рой", "рой", "рой"][x]

    elif i=="рог":output_data += ["рой", "рои", "рог", "рог", "рог"][x]

    else: output_data+=i

  return(output_data)

def corrupt_gender(data, corrupt_prob_percent=10):
  output_data=""
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]
    y=i
    if x <= corrupt_prob_percent:
      if re.findall("ро│рпН$", i): y=re.sub("ро│рпН$", "ройрпН", i)
      else: y=re.sub("ройрпН$", "ро│рпН", i)
    output_data+=y+" "
  return(output_data)

# def corrupt_tense(data):
#   output_data=""
#   for i in data.split(" "):
#     x=np.random.randint(2, size=1)[0]
#     y=i
#     if x==0:
#       if re.findall("роВродро╛ро│рпН$", i): y=re.sub("роВродро╛ро│рпН$", "ро┐ро░рпБрокроВрокро╛ро│рпН", i)
#       elif re.findall("роВродро╛ройрпН$", i): y=re.sub("роВродро╛ройрпН$", "ро┐ро░рпБрокроВрокро╛ро│рпН", i)
#       elif re.findall("роВрокро╛ро│рпН$", i): y=re.sub("ро┐ро░рпБрокроВрокро╛ро│рпН$", "роВродро╛ройрпН", i)
#       elif re.findall("роВрокро╛ройрпН$", i): y=re.sub("ро┐ро░рпБрокроВрокро╛ройрпН$", "роВродро╛ройрпН", i)
#     output_data+=y+" "
#   return(output_data)

def corrupt_pluarlity(data, corrupt_prob_percent=10):
  output_data=""
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]
    y=i
    if x<=corrupt_prob_percent:
      if re.findall("ро│рпН$", i): y=re.sub("ро│рпН$", "ро░роВрпзро│роВ", i)
      else: y=re.sub("ройрпН$", "ро░роВрпзро│роВ", i)
    output_data+=y+" "
  return(output_data)

def corrupt_thinai(data, corrupt_prob_percent=10):
  output_data=""
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]+1
    y=i
    if(x <= corrupt_prob_percent):
      if re.findall("ро╛ро│рпН$", i): y=re.sub("ро╛ро│рпН$", "родрпБ", i)
      elif re.findall("ро╛ройрпН$", i): y=re.sub("ро╛ройрпН$", "родрпБ", i)
      elif re.findall("ройрпН$", i): y=re.sub("ройрпН$", "родрпБ", i)
      elif re.findall("ро│рпН$", i): y=re.sub("ро│рпН$", "родрпБ", i)
      # elif re.findall("родрпБ$", i): y=re.sub("родрпБ$", "ро│рпН", i)
    output_data+=y+" "
  return(output_data)

def remove_random_chars(data, remove_prob_percent=10):
  output_data = ""
  for i in data:
    x=np.random.randint(100, size=1)[0]+1
    if(x > remove_prob_percent):
      output_data += i
  return output_data

def remove_random_words(data, remove_prob_percent=5):
  output_data=""
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]+1
    if(x > remove_prob_percent):
      output_data += i + " "
  return(output_data)

def repeat_words(data, repeat_prob_percentage=5):
  output_data=""
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]+1
    if(x <= repeat_prob_percentage):
      output_data += i + " "
    output_data+=i+" "
  return(output_data)

def add_random_words(data, random_words=fixed_random_words, add_prob_percentage=5):
  output_data=""
  n = len(random_words)
  if(n==0): return data
  for i in data.split(" "):
    x=np.random.randint(100, size=1)[0]+1
    idx=np.random.randint(n, size=1)[0]
    if(x <= add_prob_percentage):
      output_data += random_words[idx] + " "
    output_data+=i+" "
  return(output_data)

def merge_sentence(data, merge_prob_percent=10):
  output_data = ""
  for i in data:
    if(i == '.'):
      x=np.random.randint(100, size=1)[0]+1
      if(x > merge_prob_percent):
        output_data += " рооро▒рпНро▒рпБроорпН"
    return output_data

Tamil_Dataset = {
    'dataset_name': 'oscar',
    'dataset_subset': 'unshuffled_deduplicated_ta',
    'text_label' : 'text',
    'test_sentence': "рокрпКро┤рпБродрпБ роЪро╛ропрпНроирпНродрпБ ро╡рпЖроХрпБ роирпЗро░рооро╛роХро┐ро╡ро┐роЯрпНроЯродрпБ ЁЯШБ ?",
}

English_Dataset = {
    'dataset_name': 'glue',
    'dataset_subset': 'cola',
    'text_label' : 'sentence', 
    'test_sentence': "Hello, y'all! How are you ЁЯШБ ?"
}


if(args.language==0):
  curDataset = Tamil_Dataset
else: 
  curDataset = English_Dataset

dataset_len = args.ndata


train_dataset = load_dataset(curDataset["dataset_name"] , curDataset["dataset_subset"], split='train[:100]')
test_dataset = load_dataset(curDataset["dataset_name"] , curDataset["dataset_subset"], split='train[100:]')
print(test_dataset)
# print(dataset[0][curDataset["text_label"]])



save_dir = args.savedir+'train/'

if(not os.path.isdir(save_dir)):
  os.makedirs(save_dir)

f_org = open(save_dir + 'original.txt', 'w')
f_cor = open(save_dir + 'corrupted.txt', 'w')
# dataset_len = len(dataset)

count = 0

for data in tqdm(train_dataset, total=dataset_len):
  count += 1
  if(count > dataset_len): break
  data = data[curDataset["text_label"]]
  data_original = list(data.split(". "))

  
  for d in data_original:
    if(d=='\n' or len(d)==0):continue
    d += "\n"
    f_org.write(d)


  # data=corrupt_gender(data)
  # data=corrupt_homophones(data)
  # data=remove_random_chars(data, 10)

  # data_corrupted = list(data.split(". "))

  data=remove_random_words(data, 10)
  data=add_random_words(data, fixed_random_words, 30)
  data=repeat_words(data, 30)
  data=merge_sentence(data, 30)
  data=corrupt_thinai(data, 30)
  data=corrupt_gender(data, 30)
  data=corrupt_homophones(data)
  data=remove_random_chars(data)
  data_corrupted = list(data.split(". "))

  for d in data_corrupted:
    if(d=='\n' or len(d)==0):continue
    d += "\n"
    f_cor.write(d)

f_org.close()
f_cor.close()

save_dir = args.savedir+'dev/'


f_org = open(save_dir + 'original.txt', 'w')
f_cor = open(save_dir + 'corrupted.txt', 'w')

count = 0

for data in tqdm(test_dataset, total=dataset_len):
  count += 1
  if(count > dataset_len): break
  data = data[curDataset["text_label"]]
  data_original = list(data.split(". "))
  # print('length of original data', len(data_original))
  
  for d in data_original:
    if(d=='\n' or len(d)==0):continue
    d += "\n"
    f_org.write(d)


  data=remove_random_words(data, 10)
  data=add_random_words(data, fixed_random_words, 30)
  data=repeat_words(data, 30)
  data=merge_sentence(data, 30)
  data=corrupt_thinai(data, 30)
  data=corrupt_gender(data, 30)
  data=corrupt_homophones(data)
  data=remove_random_chars(data)
  data_corrupted = list(data.split(". "))
  # print('length of corrupted data', len(data_corrupted))

  for d in data_corrupted:
    if(d=='\n' or len(d)==0):continue
    d += "\n"
    f_cor.write(d)

f_org.close()
f_cor.close()

'''
# print("--------------------------------- Corrupted Data ---------------------------------")
# print(data)
роЗро░рпБроирпНродро╛ро│рпН
рооро╛
ро╛роо
'''